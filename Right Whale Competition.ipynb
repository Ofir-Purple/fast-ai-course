{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/ubuntu/ofir/data/rightwhale/'\n",
    "SAVED_WEIGHTS_DIR = DATA_DIR + 'saved-weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, glob\n",
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from keras import models, layers\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "# import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Right Whale Competition\n",
    "\n",
    "TODO:\n",
    "\n",
    "1. Download the data\n",
    "1. Pre-process the data\n",
    "1. Prepare a training, sample, valid, test dirs\n",
    "1. Finetune and fit the data\n",
    "1. Predict results\n",
    "1. Create a submittions file\n",
    "1. Submit to Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "\n",
    "Done in bash into DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipping this step for now. Let's start with a naive approach and see how it goes from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a training, valid, sample, test dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the CSV file to get the classes of the whales\n",
    "traindf = pd.read_csv(DATA_DIR + 'train.csv', sep=',',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the train folders based on the whale IDs\n",
    "for whale_id in tqdm(traindf['whaleID'].unique()):\n",
    "    %mkdir -p '$DATA_DIR'train/'$whale_id'/\n",
    "    \n",
    "# Make sample, valid and test dirs\n",
    "%mkdir -p '$DATA_DIR'valid/\n",
    "%mkdir -p '$DATA_DIR'test/\n",
    "%mkdir -p '$DATA_DIR'sample/train/\n",
    "%mkdir -p '$DATA_DIR'sample/valid/\n",
    "%mkdir -p '$SAVED_WEIGHTS_DIR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '/home/ubuntu/ofir/data/rightwhale/imgs/w_7812.jpg': No such file or directory\n",
      "mv: cannot stat '/home/ubuntu/ofir/data/rightwhale/imgs/w_7489.jpg': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Move images from imgs to /train/\n",
    "for row in tqdm(traindf.iterrows()):\n",
    "    whale_id = row[1]['whaleID']\n",
    "    image_name = row[1]['Image']\n",
    "    %mv '$DATA_DIR'imgs/'$image_name' '$DATA_DIR'train/'$whale_id'/\n",
    "    \n",
    "# Note that for some odd reason, I got a message that there's no such file or directory as \"w_7489.jpg\".\n",
    "# Upon further inspection, I couldn't find such a file in images.zip, and the file name does exist in train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Move the rest of the images from imgs to /test/\n",
    "%mv '$DATA_DIR'imgs/* '$DATA_DIR'test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/447 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '/home/ubuntu/ofir/data/rightwhale/train/whale_48813/w_7812.jpg': No such file or directory\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 447/447 [01:41<00:00,  4.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# Move one of each training samples to the valid folder\n",
    "for whale_id in tqdm(traindf['whaleID'].unique()):\n",
    "    image_name = traindf[traindf['whaleID'] == whale_id].iloc[0]['Image']\n",
    "    %mkdir -p '$DATA_DIR'valid/'$whale_id'/\n",
    "    %mv '$DATA_DIR'train/'$whale_id'/'$image_name' '$DATA_DIR'valid/'$whale_id'/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# Copy some of the whale IDs to the sample dirs (train & valid)\n",
    "\n",
    "sample_whale_ids = traindf.sample(10)['whaleID'].unique()\n",
    "\n",
    "for sample_id in tqdm(sample_whale_ids):\n",
    "    %cp -r '$DATA_DIR'train/'$sample_id' '$DATA_DIR'sample/train/\n",
    "    %cp -r '$DATA_DIR'valid/'$sample_id' '$DATA_DIR'sample/valid/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create the VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create helper functions for creating the network layers\n",
    "\n",
    "def FCBlock(output_dim, activation='relu', **kwargs):\n",
    "    return layers.Dense(output_dim=output_dim, activation=activation, **kwargs)\n",
    "\n",
    "def ConvBlock(model, layers_num, filters):\n",
    "    \n",
    "    for i in range(layers_num):\n",
    "        model.add(layers.ZeroPadding2D(padding=(1,1)))\n",
    "        model.add(layers.Conv2D(nb_filter=filters, nb_row=3, nb_col=3, activation='relu'))\n",
    "    \n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "#     model.add(layers.Dropout(0.5))\n",
    "\n",
    "vgg_mean = np.array([123.68, 116.779, 103.939]).reshape(3,1,1)\n",
    "def vgg_preprocessing(x):\n",
    "    \n",
    "    # Make the mean 0 relative to VGG16.\n",
    "    x -= vgg_mean\n",
    "    \n",
    "    # RGB -> BGR\n",
    "    return x[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the VGG16 network\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Lambda(vgg_preprocessing, input_shape=(3,224,224)))\n",
    "\n",
    "ConvBlock(model, 2, 64)\n",
    "ConvBlock(model, 2, 128)\n",
    "ConvBlock(model, 3, 256)\n",
    "ConvBlock(model, 3, 512)\n",
    "ConvBlock(model, 3, 512)\n",
    "# model.add(Conv2DBlock(input_shape=(224,224,3,)), filters=64)\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(FCBlock(output_dim=4096))\n",
    "model.add(FCBlock(output_dim=4096))\n",
    "model.add(FCBlock(output_dim=1000, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the weights into the model\n",
    "FILES_PATH = 'http://www.platform.ai/models/';\n",
    "fpath = get_file('vgg16.h5', FILES_PATH+'vgg16.h5', cache_subdir='models')\n",
    "model.load_weights(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune and fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# More helper functions\n",
    "def get_batches(directory, gen=ImageDataGenerator()):\n",
    "    return gen.flow_from_directory(directory, shuffle=False, batch_size=64, target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4096 images belonging to 447 classes.\n",
      "Found 447 images belonging to 447 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = get_batches(DATA_DIR + 'train/')\n",
    "valid_batches = get_batches(DATA_DIR + 'valid/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finetune the model by replacing the last layer\n",
    "\n",
    "model.pop()\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "model.add(FCBlock(output_dim=train_batches.nb_class, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store the classes\n",
    "train_batches.class_indices\n",
    "classes = list(train_batches.class_indices)\n",
    "for class_name, class_idx in train_batches.class_indices.items():\n",
    "    classes[class_idx] = class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4096/4096 [==============================] - 682s - loss: 8.7338 - acc: 0.0095 - val_loss: 10.2779 - val_acc: 0.0291\n",
      "Epoch 2/5\n",
      "4096/4096 [==============================] - 614s - loss: 8.0052 - acc: 0.0378 - val_loss: 10.3136 - val_acc: 0.0268\n",
      "Epoch 3/5\n",
      "4096/4096 [==============================] - 634s - loss: 6.7956 - acc: 0.0911 - val_loss: 9.6494 - val_acc: 0.0358\n",
      "Epoch 4/5\n",
      "4096/4096 [==============================] - 621s - loss: 5.8018 - acc: 0.1643 - val_loss: 9.7431 - val_acc: 0.0492\n",
      "Epoch 5/5\n",
      "4096/4096 [==============================] - 612s - loss: 5.0561 - acc: 0.2412 - val_loss: 9.7189 - val_acc: 0.0559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f06fbcb3750>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_batches, samples_per_epoch=train_batches.nb_sample, \n",
    "    validation_data=valid_batches, nb_val_samples=valid_batches.nb_sample, \n",
    "    nb_epoch=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the weights of the first 5 epochs of the training\n",
    "model.save_weights(filepath=SAVED_WEIGHTS_DIR + 'fit5.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4096/4096 [==============================] - 711s - loss: 4.2910 - acc: 0.3115 - val_loss: 9.7353 - val_acc: 0.0559\n",
      "Epoch 2/5\n",
      "4096/4096 [==============================] - 610s - loss: 3.7520 - acc: 0.3950 - val_loss: 9.5204 - val_acc: 0.0649\n",
      "Epoch 3/5\n",
      "4096/4096 [==============================] - 616s - loss: 3.2050 - acc: 0.4634 - val_loss: 9.4198 - val_acc: 0.0671\n",
      "Epoch 4/5\n",
      "4096/4096 [==============================] - 624s - loss: 2.7298 - acc: 0.5310 - val_loss: 9.3586 - val_acc: 0.0626\n",
      "Epoch 5/5\n",
      "4096/4096 [==============================] - 618s - loss: 2.3363 - acc: 0.5930 - val_loss: 9.3179 - val_acc: 0.0694\n"
     ]
    }
   ],
   "source": [
    "# Train again for 5 epochs and save the weights again\n",
    "model.fit_generator(\n",
    "    train_batches, samples_per_epoch=train_batches.nb_sample, \n",
    "    validation_data=valid_batches, nb_val_samples=valid_batches.nb_sample, \n",
    "    nb_epoch=5\n",
    ")\n",
    "model.save_weights(filepath=SAVED_WEIGHTS_DIR + 'fit10.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4096/4096 [==============================] - 706s - loss: 2.0433 - acc: 0.6338 - val_loss: 9.3656 - val_acc: 0.0671\n",
      "Epoch 2/5\n",
      "4096/4096 [==============================] - 610s - loss: 1.7233 - acc: 0.6978 - val_loss: 9.2965 - val_acc: 0.0761\n",
      "Epoch 3/5\n",
      "4096/4096 [==============================] - 632s - loss: 1.4779 - acc: 0.7395 - val_loss: 9.2474 - val_acc: 0.0761\n",
      "Epoch 4/5\n",
      "4096/4096 [==============================] - 608s - loss: 1.2652 - acc: 0.7832 - val_loss: 9.2071 - val_acc: 0.0738\n",
      "Epoch 5/5\n",
      "4096/4096 [==============================] - 618s - loss: 1.0805 - acc: 0.8140 - val_loss: 9.1734 - val_acc: 0.0716\n"
     ]
    }
   ],
   "source": [
    "# Train again for 5 epochs and save the weights again\n",
    "model.fit_generator(\n",
    "    train_batches, samples_per_epoch=train_batches.nb_sample, \n",
    "    validation_data=valid_batches, nb_val_samples=valid_batches.nb_sample, \n",
    "    nb_epoch=5\n",
    ")\n",
    "model.save_weights(filepath=SAVED_WEIGHTS_DIR + 'fit15.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(gen):\n",
    "    confs = model.predict_generator(gen, gen.nb_sample)\n",
    "    preds = np.argmax(confs, axis=1)\n",
    "    confs = [confs[i][preds[i]] for i in range(len(preds))]\n",
    "    \n",
    "    return preds, confs, np.array(classes)[preds]\n",
    "    \n",
    "# p1 = np.argmax(preds, axis=1)\n",
    "# conf = [preds[i][p1[i]] for i in range(len(p1))]\n",
    "# np.array(classes)[p1], p1, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds, confs, pred_classes = predict(valid_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['whale_89615', 'whale_90911', 'whale_87604', 'whale_90911',\n",
       "        'whale_86158', 'whale_90911', 'whale_89615', 'whale_89615',\n",
       "        'whale_90911', 'whale_87604'], \n",
       "       dtype='|S11'),\n",
       " ['whale_00195/w_6326.jpg',\n",
       "  'whale_00442/w_9183.jpg',\n",
       "  'whale_02411/w_2577.jpg',\n",
       "  'whale_02608/w_6600.jpg',\n",
       "  'whale_02839/w_4678.jpg',\n",
       "  'whale_03103/w_8706.jpg',\n",
       "  'whale_03227/w_6695.jpg',\n",
       "  'whale_03623/w_1617.jpg',\n",
       "  'whale_03728/w_906.jpg',\n",
       "  'whale_03935/w_3236.jpg'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_classes[:10], valid_batches.filenames[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a submittions file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Thoughts for improving:\n",
    "\n",
    "1. Crop the images\n",
    "1. Use the face detection suggestion from Kaggle.\n",
    "1. Add shuffle=True to the training examples for additional randomness\n",
    "1. Retrain more than one layer (don't skip the step of finetuning the last layer first, since otherwise the random weights of the last layer will through off the weights of the other layers).\n",
    "\n",
    "### Other tips:\n",
    "\n",
    "1. Check the model's summary\n",
    "1. Check for random correct\\incorrect predictions, The most confident correct/incorrect predictions, and the most unsure predictions.\n",
    "1. Load weights right after saving them in order to make sure they're saved correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
